---
layout: post
title:  "딥러닝의 빈도 추정과 베이지안"
date:   2020-08-20
excerpt: "Tensorflow KR에서 기억에 남는 글 남기기"
tag:
- Frequentist Probability
- Bayesian Probability
- Tensorflow KR

comments: true
---

실험을 하거나 연구를 하다 심심하면 Tensorflow KR이나 트위터 등 국내외 딥러닝 커뮤니티를 자주 보는 편이다. 오늘도 코딩을 하다 연구실 의자를 뒤로 젖힌 채로 어떤 글이 있나 보고 있었는데 TFKR의 새로운 글들에 밀려 기억에서 잊혀지기에는 아까운 글[(출처)](https://www.facebook.com/groups/TensorFlowKR/permalink/1272663176408071/)이 있어 블로그에 남겨 놓고 싶은 글이 있었다. 해당 글은 나와 이름이 같은 분의 질문으로 베이지안과 빈도주의적 관점의 차이를 묻는 질문이다.

딥러닝 연구를 하면서도 딥러닝 공부를 시작할 때 가장 처음부터 나오는 Likelihood라는 단어는 내게 뭔가 잡힐듯 잡하지 않는 단어처럼 느껴졌다. 댓글에 이 질문에 따른 답변을 보면 이해에 큰 도움이 될 것이라고 생각한다.

### 질문
안녕하세요. TF-KR!
확률론 공부하다가 궁금한 점이 생겼는데 물어볼 곳이 없어서 여기에 질문 올려봅니다.
이안 굿펠로우가 쓴 책 <Deep Learning>의 3.1절 마지막에 보면 다음과 같은 문장이 나옵니다.

"If we list several properties that we expect common sense reasoning about uncertainty to have, then the only way to satisfy those properties is to treat Bayesian probabilities as behaving exactly the same as frequentist probabilities. For example, if we want to compute the probability that a player will win a poker game given that she has a certain set of cards, we use exactly the same formulas as when we compute the probability that a patient has a disease given that she has certain symptoms. For more details about why a small set of common sense assumptions implies that the same axioms must control both kinds of probability, see Ramsey (1926)."

이 문장이 잘 이해가 안됩니다. 어렴풋이 생각하기로는 앞서 확률을 해석하는 두 가지 방법(베이지안과 빈도주의 방법)을 살펴봤는데 이 두 확률이 같은 성질지닌다는 뜻 같습니다. 보통 베이지안 관점으로 보든 빈도주의 관점으로 보든 확률의 성질이나 확률변수, 확률분포
같은 개념은 똑같이 적용된다는 의미인가요?

### 답변 1
#### 김범수님
저 나름대로 설명을 해 보았습니다. 틀린 부분이나 애매한 부분이 있다면 지적 부탁드립니다 :)
확률을 해석하는 관점에는 빈도주의 관점과 베이지안 관점이 있습니다. 빈도주의 관점에서는 event E가 발생할 확률을 random experiment(동일한 환경에서 여러 번 반복 가능한 실험을 뜻합니다)를 계속 시행함에 따라 E가 발생하는 상대적인 빈도가 수렴하는 값으로 해석합니다. 반면에, 베이지안 관점에서는 event E가 발생할 확률을 본인이 / 타인이 E가 발생할 것이라 믿는 확신의 정도로 해석합니다. 그리고 확률론은 본래 빈도주의 관점에서 고안이 되었습니다.  
하지만 기계학습에서 다루는 uncertainty는 빈도주의 관점보다 베이지안 관점이 적합합니다. 왜냐하면 기계학습에서 마주치게 되는 event는 동일한 환경에서 반복하는 것이 불가능하기 때문입니다. 예를 들어 의사가 남자 A가 감기에 걸렸을 확률이 40%라고 말했다고 해 봅시다. 이 주장은 빈도주의 관점에서 해석이 불가능합니다. 왜냐하면 (random experiment의 정의에 따라) A를 복사해서 동일한 환경에 노출 시킨 후 감기에 걸린 빈도를 측정하는 것은 불가능하기 때문입니다. 이 주장은 의사의 A가 감기에 걸렸을 것이라 확신하는 정도를 수치화 한 것이기 때문에 베이지안 관점의 주장입니다.  
하지만 책이나 논문을 보면 이러한 베이지안 관점의 확률을 (빈도주의 관점에서 고안된) 확률론으로 분석합니다. 예를 들어 특정한 증상을 가진 사람이 감기에 걸렸을 posterior 확률을 베이즈 정리 공식으로 계산합니다. 여기서 "베이지안 관점의 확률을 (빈도주의 관점에서 고안된) 확률론으로 다루어도 괜찮을까?"라는 문제가 생깁니다.  
책에서는 "그래도 괜찮다"라고 말합니다. 왜냐하면 베이지안 확률이 논리적으로 만족해야 하는 몇가지 조건을 나열해 보았을때 (if we list several properties that we expect common sense reasoning about uncertainty to have), 베이지안 확률이 나열한 조건을 만족하기 위해서는 빈도주의 확률과 똑같이 행동해야 하기 때문입니다 (then the only way to satisfy those properties is to treat Bayesian probabilities as behaving exactly the same as frequentist probabilities).  
위에서 언급한 몇 가지 조건들을 "확률의 공리(axioms of probability)"라 부릅니다. 구체적으로 세 가지 조건이 있습니다:  
(1) 사건이 발생할 확률은 0 이상이다.  
(2) 모든 사건이 발생할 확률은 1이다.  
(3) mutually exclusive 한 사건들이 발생할 확률은 각 사건이 발생할 확률의 합이다.  
이 공리는 베이지안 관점에서도 합리적입니다. 예를 들어, (1) 내일 비가 올 확률은 0 이상이며, (2) 내일 비가 오거나 오지 않을 확률은 1이고, (3) 내일 비가 오거나 눈이 올 확률은 비가 올 확률과 눈이 올 확률의 합입니다.
위의 axioms of probability를 가지고 우리는 확률론의 모든 정리를 유도할 수 있습니다. 따라서 우리는 빈도주의 확률이든 베이지안 확률이든 동일하게 확률론의 도구를 적용해도 됩니다. 더 구체적인 내용이 궁금하시다면 수리통계학이나 확률론 책을 참고하시길 바랍니다.

#### 대댓 1
자세한 설명 정말 감사드립니다! 덕분에 이해 안되던 부분이 잘 정리가 됐습니다. 🙂
하나 추가적으로 궁금증이 생겼는데, 그러면 빈도주의 확률로 해석한 확률값 끼리 계산을 한 결과를 베이지안 관점으로 해석해도 되는건가요?

#### 대댓 2
제 설명에서 모호한 부분이 있었군요 (방금 수정했습니다). 빈도주의 확률로 해석한 확률값 끼리 계산을 한 결과를 베이지안 관점으로 해석하는 것은 안 된다고 생각합니다. Deep Learning 책에서 주장하는 바는 빈도주의 확률이든 베이지안 확률이든 동일하게 확률론의 도구를 적용해도 된다는 것이지, 빈도주의 관점과 베이지안 관점이 동일하다는 것은 아니기 때문입니다.

#### 대댓 3
아! 그러면 해석방법을 베이지안으로 했으면 계속 베이지안 관점으로 해야하고, 빈도주의 관점으로 했으면 일관되게 빈도주의 관점으로 하되, 확률의 성질이나 확률끼리 계산하는 방법 자체는 두 관점 모두 (빈도주의관점에서 고안된) 확률론으로 다루면 된다는 말씀이신가요?

#### 대댓 4
네 맞습니다.

### 고현웅 님
확률을 바라보는 관점은 크게 빈도주의와 베이지안, 두가지가 있습니다. 이 두 가지가 사실 잘 와닿지 않는데, 머신러닝의 학습 프로세스와 베이즈 정리를 알면 이 두가지를 직관적으로 이해할 수 있습니다.  
빈도주의의 입장에서는 가설(H)를 세워놓고, 반복 실험을 통해 P(D|H)를 구합니다. 가령 "동전을 던지면 앞면이 나올 확률은 50%다." 라는 가설을 세우고, 실제로 100번, 1000번 실험해본 뒤, 가설을 채택하거나 기각합니다. 즉, 가설(H)을 고정해놓고, 데이터가 발생할 확률을 생각합니다. 이런 방식으로 가설이 주어질때 데이터가 발생할 확률을 "확률"로써 생각하는게 빈도주의적 관점이고, 약간 80년대 전문가 시스템 (rule based 시스템)이랑 비슷합니다. if-then 구조를 사람이 세워놓고 잘 맞추는지 보는것이죠. 잘 못맞추면 사람이 직접 if-then 구조를 바꿔야 하고.. 그리고 사실 이런 방식이 우리가 지금껏 생각해왔고, 배워왔던 확률이죠. 확률론 수업듣다보면 맨날 동전이랑 주사위 던지잖아요.  
그에 반해 베이지안은 반대로 데이터가 주어질때 (고정되었을때) 가설의 신뢰도 생각합니다. 우리가 일상에서 접하는 일들은 저렇게 100번 1000번 10000번씩 반복 시행하지 못하는 경우도 많거든요. 쉽게 생각해서 질병 예측모델을 만드는데 환자 데이터를 100샘플, 1000샘플 10000샘플 모으는 것은 힘드니까요. 이럴때 적용하는게 베이지안인데, 어느정도 데이터가 고정되었을때 가장 잘 맞출만한 모델을 찾는것이라고 생각할 수 있습니다. 즉 가장 높은 P(H|D) = Likelihood(모델의 신뢰도 = 모델의 뛰어난 정도 = 한자로 하면 우도, 뛰어날 우)를 구하는 과정이 베이지안의 기본적인 사고방법입니다.  
우리가 머신러닝할때 학습데이터셋(D)은 고정되어있고, 모델의 파라미터(H)를 바꾸지 않습니까? 그게 곧 베이지안인거죠. 흔히 로지스틱 회귀같은 모델로 분류 태스크를 풀때 크로스 엔트로피 Loss를 minimize 하게 되는데, 그건 곧 negative likelihood를 minimize하는것이고, 그 말은 positive likelihood = P(H|D)를 maximize 하는 과정이죠. 그리고 모델의 성능을 테스트 할때는 다시, 모델을 고정하고 새로운 데이터를 넣어보죠? 그땐 다시 P(D|H)를 구하게 되는데, 데이터(D)로 인해 가설(H)이 업데이트 되었기 때문에, 이때 구하는 확률(정확도=accuracy)는 사후 확률이 됩니다. 가령, "10개의 클래스중 1개의 클래스를 맞출 확률은 10%이다." 와 같은 사전확률이 있다면 epoch마다 데이터를 보면서 점점 더 높은 likelihood로 업데이트 하면서 사전확률 x likelihood = 사후확률을 높혀가는거고, 이전의 accuracy는 다시 사전확률이 되고, epoch이 변하면서 새로운 사후확률이 계산되어서 계속 모니터에 찍히는거죠. 즉, 베이지안들은 데이터를 기반으로 가설의 신뢰을 점점 업데이트 해나가는것을 "확률"로써 생각하게 됩니다.  
그런데 책에서는 머신러닝(베이지안 확률)을 그냥 우리가 이전에 생각했던 빈도주의에서 고안된 기법들로 다루어도 과연 괜찮은 것인가에 대해 이야기하는데요. 책에서는 두 확률이 만족해야하는 몇가지 논리 들이 같기 때문에 상관없다고 이야기합니다. 이런 몇가지 논리(~공리 = axiom)에 대해 이야기하려면 확률의 역사를 알 필요가 있습니다.  
확률의 역사를 따라가보면, 과거에는 확률이라는 것이 애매모호한 개념이다가 Laplace라는 수학자에 의해 n(A) / n(S)라고 정의됩니다. 여기서 A는 특정 사건이고, S는 전체 사건(표본공간)을 의미하는데요. 잘 아시다시피 사건은 집합이기 때문에 A가 발생할 확률을 A가 발생하는 경우의 수(=A의 원소의 수)를 전체 사건이 발생하는 경우의 수(=S의 원소의 수)로 나눈 것으로 생각하게 됩니다.
그런데 이런 경우는 동전던지기나 주사위던지기처럼 근원사건(원소가 1개인 사건)이 발생할 확률이 같을때만 잘 맞는데요. 가령 윷놀이는 실제로는 도개걸윷모가 나올 확률이 전부 다른데, 위 처럼 적용하면 도가 나올 확률도 1/5, 윷이 나올 확률도 1/5가 되어버리기 때문에 잘 안맞죠. 양궁을 할때 10점(만점)을 맞을 확률을 구하려면 10점 영역에 맞을 사건의 원소의 수를 구해야하는데, 공간상에 화살이 위치하는 경우는 윈소의 수를 세기엔 뭔가 애매합니다.(집합으로 모델링하기에 애매합니다) 그래서 이후에 등장한 기하학적 확률, 통계적 확률(대수의 법칙) 등의 확률을 사용다가, 소련의 수학자인 콜모고로프에 의해 새롭게 정의됩니다.
콜모고로프는 사건(집합)은 어떠한 집합연산을 하더라도, 그들의 멱집합(시그마, 모든 사건을 부분집합으로서 포함하는 집합)에 포함 되어야한다고 가정하고 나서 아래 세개의 공리를 정의하는데요.  
(1) 확률이라는건 반드시 0 이상이며, (2) 모든 사건이 발생할 확률은 1이고, (3) 교집합이 존재하지 않는 특정 사건들의 확률합은 그 확률의 합에 해당하는 사건이 발생할 확률과 같다. 라는 세가지 공리만 만족한다면 무엇이든 확률로서 부를수 있다고 한것입니다. 이 콜모고로프의 공리적 확률 때문에 우리가 확률밀도함수(PDF)의 넓이도 확률이라고 부를 수 있는것이죠.  
책에서는 빈도주의적 확률도, 베이지안 확률도 결국 생각해보면 콜모고로프의 공리를 따르는 확률이기 때문에 둘을 동일한 방식으로 취급해도 큰 문제가 없다고 이야기 하고있습니다. 이것저것 장황하게 설명했는데, 만약 틀린부분이 있다거나 더 궁금하신 내용이 있으시면 답글을 달아주시길 바랍니다.  

#### 대댓 1
확률공간에 대한 설명에 약간의 오류가 있는 것 같아서 첨언합니다. 표본 공간이 유한집합이면 사건 공간(시그마 / 시그마 대수 / 시그마 필드라고도 불립니다)은 멱집합이 맞습니다. 하지만 표본 공간이 실수집합이면 사건 공간은 멱집합이 아닙니다. 왜냐하면 실수의 부분집합 중 확률을 부여할 수 없는 집합(non-measurable set)도 있기 때문입니다. 따라서 표본 공간이 실수집합이면 일반적으로 사건 공간을 (1) 모든 열린 구간을 포함하며 (2) complement, countable intersection, countable union에 대해 닫혀있는 실수의 부분집합의 collection 중 가작 작은 collection으로 둡니다. (이 사건 공간을 보렐 시그마-대수라고 부릅니다.) 이렇게 정의를 해야 콜모고로프의 공리가 모순 없이 성립하기 때문입니다.

#### 대댓 2
1. 빈도주의와 베이지안 설명  
저 같은 컴퓨터 엔지니어링 전공자들은 학교에서 수학을 엄청 깊게 배우지는 않습니다. 저는 학부에서 4년동안 컴퓨터의 내부 구조, 코드를 예쁘고 효율적으로 잘 짜는방법 같은 것을 깊게 배웠구요. 수학은 전체 커리큘럼 중에서 6학점 내지 많으면 12학점 정도만 듣게 됩니다. 그래서 저는 위에 댓글 남겨주신 김범수님처럼 수학적인 내용을 완벽하게 깊게 내용을 아는건 아니지만, 직접 엔지니어링을 하면서 필요한 수준으로, 경험을 통해서 직관적으로 이해하게 되는 경우가 많더라구요. 머신러닝 학습 과정과 베이즈 정리의 관계는 그런 경우이구요. 그러니까 어느 책에서 내용을 발췌하거나 그런게 아니라 제가 통계적 검정, 머신러닝을 직접 해보면서 느낀 내용을 말씀드리면 이론적으로 설명하는 것 보다 훨씬 이해하기 쉬우실테니까 그런 방식으로 설명드린 것이구요. 제가 작성자님의 전공이 사이언스인지 엔지니어링인지 모르니까요.  
저 같은 경우는 빈도주의, 베이지안 관련 내용들은 책보다는 아래같은 영상들에서 영감을 많이 받았습니다. 학습의 소스가 꼭 책만 있는 건 아니니까요 ㅎㅎ 최프란님 (워싱턴대학 교수님) 영상은 그 뒤에 이어지는 베이지안 시리즈를 다 보면 좋습니다. 플레이리스트가 너무 길어서 저것만 링크합니다.  
https://www.youtube.com/watch?v=Gpi6Uuw6DJM...
https://www.youtube.com/watch?v=HZGCoVF3YvM
https://www.youtube.com/watch?v=Y4ecU7NkiEI
https://www.youtube.com/playlist...
결과적으로 제가 공부한 내용에 따르면 빈도주의와 베이지안의 결정적 차이는 "사전확률"을 기반으로 "사후확률"을 계속해서 업데이트 하는가, 아니면 likelihood가 최대인 hypothesis를 찾으려고 실험을 계속 반복하는가에 대한 차이이며, 빈도주의의 경우 실험결과는 hypothesis에 대한 증거로만 사용될뿐, hypothesis 자체를 지속적으로 업데이트 하거나 그렇지는 않습니다. 그런데 이렇게 설명하면 수학적 베이스가 깊지 않은 엔지니어링 전공자들은 직관적으로 이해하기 어렵기 때문에 (제가 그랬습니다) rule base와 machine learning으로 예시를 들어서 설명해드린거에요.  
참고로 베이즈 정리의 식을 Log 형식으로 표현한 것이 Softmax , Sigmoid 함수이고, 이들은 특히 딥러닝이나 로지스틱 회귀, SVM 등의 모델에서 분류 태스크를 풀때 Classifier의 최종적인 Activation Function으로 활용 되며, 추정한 파라미터와 입력의 연산 결과가 Logit으로 쓰이기 때문에 (Activation Function과 Logit에 대해 잘 모르신다면 회귀분석방법 중에 Generalized Linear Models처럼 이해하시면 됩니다) 적어도 제가 예시로 한 분류 태스크의 경우 위에 설명한 내용을 틀린 내용이라고 말할 수는 없을 겁니다.  
2. 확률 역사 설명  
확률의 역사에 대한 설명은 되게 유명한거라.. 대부분 대학 확률론 교재에 자주 등장하는 내용이니 검색해보시면 관련 내용을 쉽게 접하실 수 있으실거라고 생각됩니다. 답변이 도움이 되셨다면 좋겠네요!
